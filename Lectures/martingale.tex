\documentclass[main]{subfiles}

\begin{document}
%Set chapter counter as week-1
\chapMart{10}{Discrete Time Martingales}
%Set chapter name

\lectMart{4}{January 27, 2023}{Siva Athreya}{Abhiti Mishra, Devesh Bajaj}

Origin is from horse-racing (betting system). The dictionary meaning of the word `martingale' is the harness of a horse. \\
Let $\{Z_n\}_{n \geq 1}$ is a sequence of random variables on $(\Omega, \mathcal{F}, \mathbb{P})$. \\
\begin{definition} A sequence of random variables $\{Z_n\}_{n \geq 1}$ is said to be a Martingale if
    \begin{equation} \label{eq:mar-def}
        \mathbb{E} (Z_{n}|Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1 )= z_{n-1} ~~\forall ~ n \geq 2
    \end{equation} \end{definition}

Things to understand- conditional expectation for discrete and conditional random variable \cite{AST-2016}.\\
Things we will explore-
\begin{enumerate}
    \item Examples of $\{Z_n\}_{n \geq 1}$ that are martingales.\\
    \item How different are martingales from iid sequences and markov chains? \\
    \item How to interpret \ref{eq:mar-def}?
\end{enumerate}

\ex $\{S_n\}_{n \geq 1}$ and $S_0 \equiv 0$.
\begin{equation*}
    X_i=
    \begin{cases}
        1 ,  & w.p ~~ 1/2 \\
        -1 , & w.p ~~1/2  \\
    \end{cases}
\end{equation*}
$$S_n=\sum_{i=1}^n X_i$$
Let $s_{n-1},s_{n-2}, \ldots, s_1 \in \Z$ such that $\mathbb{P} (S_{n-1}=s_{n-1}, \ldots, S_1=s_1)>0$
\begin{align*}
    \mathbb{E} (S_n |S_{n-1}=s_{n-1}, \ldots, S_1=s_1) & = \sum_{k \in \Z} k \mathbb{P} (S_n =k |S_{n-1}=s_{n-1}, \ldots, S_1=s_1)                                                                        \\
                                                       & = \sum_{k \in \Z} k \frac{\mathbb{P}(S_n =k ,S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}{\mathbb{P}(S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}                    \\
                                                       & = \sum_{k \in \Z} k \frac{\mathbb{P}(S_{n-1} +X_n =k ,S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}{\mathbb{P}(S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}           \\
                                                       & = \sum_{k \in \Z} k \frac{\mathbb{P}(X_n =k -s_{n-1} ,S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}{\mathbb{P}(S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}           \\
                                                       & = \sum_{k \in \Z} k \frac{\mathbb{P}(X_n =k-s_{n-1}) \mathbb{P}(S_{n-1}=s_{n-1}, \ldots, S_1=s_1)}{\mathbb{P}(S_{n-1}=s_{n-1}, \ldots, S_1=s_1)} \\
                                                       & = (s_{n-1}+1) \mathbb{P} (X_n=-1)+ (s_{n-1}-1) \mathbb{P} (X_n=1)                                                                                \\
                                                       & = (s_{n-1}+1) \frac{1}{2}+ (s_{n-1}-1) \frac{1}{2} =s_{n-1}                                                                                      \\
\end{align*}
Note that the summations here are ``finite'' sums.\\
As $s_{n-1},\ldots, s_1 \in \Z$ were arbitrary, $\{S_n\}_{n \geq 1}$ is a martingale.\\

\ex $\{X_i\}_{i \geq 1}$ be an iid sequence on $(\Omega, \mathcal{F}, \mathbb{P})$. Let
$Z_n= \prod_{i=1}^n X_i$
and Range($Z_n$) $\subset \R ~~\forall ~~ n \geq 1$. \\
Let $z_{n-1}, \ldots, z_1 \in \R$ such that $\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)>0$. Then

\begin{align*}
    \mathbb{E} (Z_n |Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1) & = \sum_{k \in Range(Z_n)} k \mathbb{P} (Z_n=k |Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)                                                                           \\
                                                       & = \sum_{k \in Range(Z_n)} k \frac{\mathbb{P} (Z_n=k ,Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}{\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}                     \\
                                                       & =  \sum_{k \in Range(Z_n)} k \frac{\mathbb{P} (Z_{n-1}X_n=k ,Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}{\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}             \\
                                                       & =  \sum_{k \in Range(Z_n)} k \frac{\mathbb{P} (z_{n-1}X_n=k ,Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}{\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}             \\
                                                       & =  \sum_{k \in Range(Z_n)} k \mathbb{P}(Z_{n-1}X_n=k ) \frac{\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)}{\mathbb{P} (Z_{n-1}=z_{n-1}, \ldots, Z_1=z_1)} \\
                                                       & = \sum_{u \in S^1, Range(X_n)=S^1} u z_{n-1} \mathbb{P} (X_n=u)                                                                                            \\
                                                       & = z_{n-1} \mathbb{E} [X_n] =z_{n-1}                                                                                                                        \\
\end{align*}
Note that the sums here might be infinite. In the last step we assume $\mathbb{E}[X_i]=1$. Now since $\{z_i\}_{i=1}^{n-1}$ were arbitrary, $\{Z_n\}_{n \geq 1}$ is a martingale. \\

\ex
\begin{equation*}
    X_i=
    \begin{cases}
        2 , & w.p ~~ 1/2 \\
        0 , & w.p ~~1/2  \\
    \end{cases}
\end{equation*}
Then $\mathbb{E} (X_i)=1$. Therefore, $Z_n= \prod_{i=1}^n X_i$ is a martingale. Range ($Z_n$)= $\{2^n,0\}$. Note that the mean stays constant and
$$\mathbb{P}(Z_n=0)=1-\frac{1}{2^n}$$
$$\mathbb{P}(Z_n=2^n)=\frac{1}{2^n}$$
\textbf{Intuition-} The first equation shows that the martingale takes a very low value with very high probability and the second one shows that it takes a very large value with very low probability\\
Idea behind Markov Chains -
$$``X_n | X_{n-1}, \ldots, X_1" \,{\buildrel d \over =}\, X_n|X_{n-1}$$
Idea behind Martingales -
Expected value of $Z_n$ conditioned on the past depends only on $Z_{n-1}$. $\{Z_n\}_{n \geq 1}$ in law could depend on the entire past!

\lectMart{5}{February 3, 2023}{Siva Athreya}{Atreya Choudhury, Ankan Kar}

We define $f: D \subseteq \R^{n-1} \to \R$ where
$$f(z_1, z_2, \ldots, z_{n-1}) = \E[Z_n|Z_{n-1}=z_{n-1},Z_{n-2}=z_{n-2},\ldots,Z_1=z_1]$$
Define $Y_n: \Omega \to \R$ where
\begin{equation}
    Y_n(\omega) \coloneqq f(Z_1(\omega), Z_2(\omega), \ldots, Z_{n-1}(\omega))
    \label{eq:y_n}
\end{equation}
You can check that $\{Y_n\}$ is a random variable.
\begin{property} Some properties of $\{Y_n\}$
    \begin{enumerate}
        \item
              $A \coloneqq \{Z_{n-1}=z_{n-1},Z_{n-2}=z_{n-2},\ldots,Z_1=z_1\}$

              $$\omega \in A \implies Y_n(\omega) = f(z_1, z_2, \ldots, z_{n-1})$$
        \item
              $L \coloneqq \{Y_n \leq c\} = \{f(Z_1, Z_2, \ldots, Z_{n-1}) \leq c\}$

              $$L \in \SA_{n-1} \equiv \text{observable events upto }n-1$$
    \end{enumerate}
    \eqref{eq:y_n} $\iff \{Y_n\}$ has the above two properties

    \vspace{2em}
    If $\{Z_n\}$ is martingale, $Y_n = Z_{n-1}$
\end{property}
\begin{lemma}
    Let $\{Y_n\}_{n\geq 1}$ be martingale. Then,
    $$\forall\;1\leq i\leq n,\;\E[Z_n|Z_i,Z_{i-1},\ldots,Z_1] = Z_i$$
    \begin{proof}
        We fix i and prove by induction on n.\\
        We look at n = i+1. By martingale property,
        \begin{equation*}
            \E[Z_{i+1}|Z_i,Z_{i-1},\ldots,Z_1] = Z_i
            \label{eq:mart_basic}
        \end{equation*}
        Let $k>0$ and the statement hold for $n=i+k$.
        We look at $n=i+k+1$
        \begin{align*}
             & \hspace{1.4em}\E[Z_{i+k+1}|Z_i,Z_{i-1},\ldots,Z_1]                                     \\
             & = \E[\E[Z_{i+k+1}|Z_{i+k},Z_{i+k-1},\ldots,Z_1]|Z_i,Z_{i-1},\ldots,Z_1]                \\
             & = \E[Z_{i+k}|Z_i,Z_{i-1},\ldots,Z_1] \hspace{3em} [\text{using }\eqref{eq:mart_basic}] \\
             & = Z_i
        \end{align*}
        where the last equality is obtained from the induction hypothesis
    \end{proof}
\end{lemma}
The property used in the first equality is called the Tower property. We now formally state and prove the same.
\begin{property}[Tower Property]
    $$\E[\E[X|Y,Z]|Y] = E[X|Y]$$
    \begin{proof}
        $$\E[\E[X|Y,Z]|Y] = E[h(Y,Z)|Y] = k(Y)$$
        Let $y \in \R$ such that $\P(Y=y) > 0$
        \begin{align*}
            k(y) & = E[h(Y,Z)|Y]                                                                                                     \\
                 & = \sum_{\substack{m \in \Range(Y)                                                                                 \\ t \in \Range(Z)}} h(m,t)\P(Y=m,Z=t|Y=y) \\
                 & = \sum_{t \in \Range(Z)} h(y,t)\P(Z=t|Y=y)                                                                        \\
                 & = \sum_{t \in \Range(Z)} \sum_{k \in \Range(X)} k \P(X=k|Y=y,Z=t)\P(Z=t|Y=y)                                      \\
                 & = \sum_{t \in \Range(Z)} \sum_{k \in \Range(X)} k \frac{\P(X=k,Y=y,Z=t)}{\P(Y=y,Z=t)} \frac{\P(Z=t,Y=y)}{\P(Y=y)} \\
                 & = \sum_{k \in \Range(X)} \sum_{t \in \Range(Z)} k \frac{\P(X=k,Y=y,Z=t)}{\P(Y=y)}                                 \\
                 & = \sum_{k \in \Range(X)} k \frac{\P(X=k,Y=y)}{\P(Y=y)}                                                            \\
                 & = \E[X|Y=y]
        \end{align*}
    \end{proof}
\end{property}

\lectMart{6}{February 17, 2023}{Siva Athreya}{Aniket Sen, Arun Sharma}

$\{Z_n\}$ is a Martingale

$E[Z_n|Z_i, Z_{i-1}, ... , Z_1]=Z_i$ where $1 \leq i \leq n$

$E[Z_n]=E[Z_1]$

\section{Stopping time and Stopped process}
\begin{definition}
    Let $(\Omega, A, \P)$ be a probability space on which $\{Z_n\}_{n\geq 1}$ is defined.

    $\mathscr{A}_k$= events determined by $Z_1, Z_2, ... ,Z_k$.

    $T: \Omega \longrightarrow \mathbb{N} \cup \{\infty\}$ is called a \textbf{stopping time} for $\{Z_n\}_{n\geq 1}$ if $\{T=k\} \in \mathscr{A}_k$, i.e.  $\1_{T=k}=$ "function" of $Z_1, Z_2, ... ,Z_k$.
\end{definition}

\begin{definition}
    for any stopping time T, we define the \textbf{stopped process}:

    $Z_n^T(w) = Z_{n \wedge T(w)}(w)=
        \begin{cases}
            Z_n \text{ if }n<T \\
            Z_T \text{ if } n \geq T
        \end{cases}$
\end{definition}

\begin{theorem}
    Given a sequence of random variables $\{Z_n\}_{n\geq 1}$ and $T: \Omega \longrightarrow \mathbb{N} \cup \{\infty\}$, a stopping time of $\{Z_n\}_{n\geq 1}$. Then $\{Z_n^T\}_{n\geq 1}$ is a martingale iff $\{Z_n\}_{n\geq 1}$ is a martingale
\end{theorem}

\textit{Idea of the proof:}
$\E(Z_n^T| Z_{n-1}^T, ..., Z_1^T)= \E(Z_{n-1}^T)$

Take $Z_1 = z_1, ... , Z_{n-1} = z_{n-1} \rightarrow$ determine if T has happened by time n-1 or not

$\rightarrow$ if $T \geq n,$  $Z_n^T= Z_n$

\hspace{4mm}	if $T < n, $  $Z_n^T= z_{n-1}$ $\square$



Let $\{X_i\}, X, Y, Z$ be discrete random variables.

\begin{equation}
    \E[Y| X=x_1] = \sum_{k \in Range(Y)}k\P(Y=k| X=x_1)
\end{equation}

\begin{equation}
    \E[Y| X_1=x_1, ..., X_n=x_n] = \sum_{k \in Range(Y)}k\P(Y=k| X_1=x_1, ..., X_n=x_n)
\end{equation}
where $\E[Y| X_1=x_1, ..., X_n=x_n]\equiv f(x_1, x_2,..., x_n)$

$ f: \prod_{i=i}^n Range(X_i) \rightarrow \R$

\begin{eqnarray}
    \E[Y| X_1, ..., X_n](\omega) = \sum_{x \in Range(X_i)}k\E(Y=k| X_1=x_1, ..., X_n=x_n)\mathbf{1}_{(X_1=x_1, ..., X_n=x_n)}(\omega)
\end{eqnarray}
where $\E[Y| X_1, ..., X_n] \equiv \E[Y| \mathscr{A}_n]$, i.e. events observable by time n.


\section{Tower Property}
Let $\mathscr{A}_n \subset \mathscr{A}_m $, $n \leq m$
then $\E[E[Y|\mathscr{A}_m]|\mathscr{A}_n]= \E[Y|\mathscr{A}_n]$
\section{Markov property and Strong Markov Property}
Property for $\{X_n\}$ random walk on $(\Gamma,y)$.

$\Omega=\mathbb{V}^{\mathbb{Z_+}}$.

$X_n: \Omega \rightarrow \mathbb{V}$.

$X_n(\omega)= \omega(n)$.

$\mathscr{A}_n$= events determined by $X_1, ..., X_n$.

$\P^x(X_0=x_0, X_1=x_1,..., X_n=x_n)= \1_{x}(x_0)\prod_{i=0}^n \mathscr{P}(x_{i-1}, x_i)$

$\mathscr{P}(x,y)=\frac{\mu_{xy}}{\mu_y}$

$\xi \rightarrow$ random variable that is determinable by $\mathscr{A}_n$ i.e. $\xi=g(X_1, X_2,..., X_n$ for some g.

$\forall k \geq 1$, $\theta_k: \Omega \rightarrow \mathbb{V}^{\mathbb{Z_+}}$, $\theta_k(\omega)= (\omega(k), \omega(k+1),...)$

Let $\eta: \Omega \rightarrow \R$ be any random variable.

$\E[\xi"\eta$ after time $n"|\mathscr{A}_n]=\E[\xi\E^{X_n}[\eta$ after time $n"]]$

\textbf{Markov Property:}
\begin{equation}
    \E[(\xi)\times (\eta.\theta_n)|\mathscr{A}_n ]=\E[\xi\E^{X_n}[\eta]]
\end{equation}

\textbf{Strong Markov Property:}

T is a stopping time of $\{X_n\}_{n\geq 1}$.

$\mathscr{A}_n \equiv $events determined by time T.

if $\xi$ is determinable by time T, then
\begin{equation}
    \E[(\xi)\times (\eta.\theta_T)|\mathscr{A}_T ]=\E[\xi\E^{X_T}[\eta]]
\end{equation}

\end{document}
